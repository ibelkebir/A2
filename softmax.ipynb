{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adelie', 'Chinstrap', 'Gentoo'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Collection\n",
    "penguins = pd.read_csv('penguins.csv')\n",
    "\n",
    "#clearing missing values\n",
    "penguins = penguins.loc[(penguins['sex']!='?') & (penguins['sex']!='_')]\n",
    "#display(penguins)\n",
    "\n",
    "penguinsy_non_cat = np.array(penguins.iloc[:,0])\n",
    "#print(penguinsy_non_cat)\n",
    "\n",
    "\n",
    "\n",
    "#one-hot encoding for all the categorical variables\n",
    "dummies1 = pd.get_dummies(penguins.island)\n",
    "#display(dummies1)\n",
    "dummies2 = pd.get_dummies(penguins.sex)\n",
    "#display(dummies2)\n",
    "dummies3 = pd.get_dummies(penguins.species)\n",
    "#display(dummies3)\n",
    "\n",
    "penguins = pd.concat([penguins,dummies1,dummies2,dummies3],axis='columns')\n",
    "penguins = penguins.drop(['species', 'island', 'sex'], axis='columns')\n",
    "display(penguins)\n",
    "\n",
    "penguinsX= penguins.iloc[:, :9]\n",
    "penguinsy= penguins.iloc[: , 9:]\n",
    "print(\"penguinsX\")\n",
    "display(penguinsX)\n",
    "print(\"penguinsy\")\n",
    "display(penguinsy)\n",
    "\n",
    "penguinsX = np.array(penguinsX)\n",
    "penguinsX = penguinsX.astype('float64')\n",
    "penguinsy = np.array(penguinsy)\n",
    "penguinsy = penguinsy.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Optimizer\n",
    "\n",
    "softmax = lambda z: np.exp(z - z.max(axis=1, keepdims = True)) / np.exp(z - z.max(axis=1, keepdims = True)).sum(axis=1)[:,None]\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    def __init__(self, batch_size=10, learning_rate=0.1, momentum=0, max_iters=1e4, epsilon=1e-8, record_history=False, l1=0, l2=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.record_history = record_history\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        if record_history:\n",
    "            self.w_history = []                 #to store the weight history for visualization\n",
    "            \n",
    "    def optimize(self, x, y):\n",
    "        N,D = x.shape\n",
    "        w = np.zeros((y.shape[1],D))\n",
    "        grad = np.inf\n",
    "        t = 0\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            grad_prev = grad\n",
    "            grad = self.gradient(x, y, w) + self.l1 * np.column_stack([np.sign(w[:,1:]),np.zeros(w.shape[0])]) + self.l2 * np.column_stack([w[:,1:],np.zeros(w.shape[0])])  # compute the gradient, apply l1 or l2 regularization if specified\n",
    "            if t > 0:\n",
    "                grad = self.momentum * grad_prev + (1 - self.momentum) * grad\n",
    "            if self.learning_rate == 0:           # special case, use decreasing learning rate\n",
    "                w = w - (1 / (t + 1)) * grad      # weight update step using 1 / (t+1)\n",
    "            else:\n",
    "                w = w - self.learning_rate * grad         # weight update step using specified learning rate\n",
    "            if self.record_history:\n",
    "                self.w_history.append(w)\n",
    "            t += 1\n",
    "        return [w, t, np.linalg.norm(grad)]\n",
    "    \n",
    "    def gradient(self, x, y, w):\n",
    "        index = np.random.choice(x.shape[0], self.batch_size, replace=False) # choose a minibatch based on size\n",
    "        x_batch = x[index]\n",
    "        y_batch = y[index]\n",
    "        z = softmax(x_batch@w.T) - y_batch\n",
    "        return z.T@x_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function\n",
    "\n",
    "def cost_fn(x, y, w):                                                   \n",
    "    z = x@w.T\n",
    "    J = -((y * z).sum(axis=1) - (z.max(axis=1) + np.log(np.exp(z - z.max(axis=1, keepdims=True)).sum(axis=1)))).mean()\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Classifier\n",
    "\n",
    "class softmax_classifier:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, x, y, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        N = x.shape[0]\n",
    "        x = np.column_stack([x,np.ones(N)])\n",
    "        results = optimizer.optimize(x, y)\n",
    "        self.w = results[0]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'terminated after {results[1]} iterations, with norm of the gradient equal to {results[2]}')\n",
    "            print(f'the weight found: {self.w}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        Nt = x.shape[0]\n",
    "        x = np.column_stack([x,np.ones(Nt)])\n",
    "        z = softmax(x@self.w.T)\n",
    "        yh = z.argmax(axis=1)           #predict output\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8083333333333333\n",
      "0.8\n",
      "0.8579387186629527\n",
      "0.7771587743732591\n",
      "0.7743732590529248\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Optimization\n",
    "digits=load_digits()\n",
    "X = digits.data\n",
    "targets = digits.target\n",
    "y = np.zeros((X.shape[0], 10))\n",
    "for i in range(X.shape[0]):\n",
    "    y[i][targets[i]] = 1\n",
    "for train, test in KFold().split(X):\n",
    "    cl = softmax_classifier()\n",
    "    optimizer = Optimizer(learning_rate = 0, momentum = 0.9, l1=0.1, l2=0.1)\n",
    "    cl.fit(np.array([X[i] for i in train]), np.array([y[i] for i in train]), optimizer)\n",
    "    print(accuracy_score([targets[i] for i in test], cl.predict(np.array([X[i] for i in test]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softmax_score(X_train, X_test, y_train, y_test, optimiser):\n",
    "    sf= softmax_classifier().fit(np.array(X_train), np.array(y_train), optimiser)\n",
    "    #print(\"X_test is this:\")\n",
    "    #print(np.array(X_test))\n",
    "    predictions = sf.predict(np.array(X_test))\n",
    "    print(\"predictions!!\")\n",
    "    print(predictions)                           \n",
    "    print(\"true labels!\")\n",
    "    print(np.array(y_test))\n",
    "    return accuracy_score(np.array(y_test), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_k_fold(X, y, y_non_cat, optimiser, k=5):\n",
    "    stkf = StratifiedKFold(n_splits=k)\n",
    "    scores=[]\n",
    "    for train_index, test_index in stkf.split(X, y_non_cat):\n",
    "        X_train, X_test, y_train = X[train_index], X[test_index], y[train_index]       \n",
    "        y_test = []      \n",
    "        \n",
    "        for i in test_index:\n",
    "            #print(y[i])\n",
    "            if np.array_equal(y[i],[1,0,0]):\n",
    "                y_test.append(0)\n",
    "            elif np.array_equal(y[i],[0,1,0]):\n",
    "                y_test.append(1)\n",
    "            elif np.array_equal(y[i],[0,0,1]):\n",
    "                y_test.append(2)\n",
    "        y_test= np.array(y_test)\n",
    "        \n",
    "     #   print(\"this is y_test\")\n",
    "      #  print(y_test)\n",
    "        scores.append(get_softmax_score(X_train, X_test, y_train, y_test, optimiser))\n",
    "        \n",
    "    return np.average(np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorez = {}\n",
    "b_sizes = [15]\n",
    "lrs= [0.01]\n",
    "betas=[0.9]\n",
    "\n",
    "for bs in b_sizes:\n",
    "    for lr in lrs:\n",
    "        for beta in betas:\n",
    "            optimizer = Optimizer(batch_size=bs, learning_rate = lr, momentum = beta)\n",
    "            scorez[str(bs)+'_'+str(lr)+'_'+str(beta)]= softmax_k_fold(penguinsX, penguinsy, penguinsy_non_cat, optimizer, k=5)\n",
    "            print(f'done with size {bs}, lr {lr}, beta {beta}')\n",
    "\n",
    "            \n",
    "print(scorez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
